{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3AdgL6PxWAxdV1mXcifMJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Besttenson/5ARIP10/blob/Brax_training/brax_training_ppo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjnFT6vzCkdu"
      },
      "outputs": [],
      "source": [
        "#@markdown ## ⚠️ PLEASE NOTE:\n",
        "#@markdown This colab runs best using a GPU runtime.  From the Colab menu, choose Runtime > Change Runtime Type, then select **'GPU'** in the dropdown.\n",
        "\n",
        "import functools\n",
        "import jax\n",
        "import os\n",
        "\n",
        "from datetime import datetime\n",
        "from jax import numpy as jp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython.display import HTML, clear_output\n",
        "\n",
        "try:\n",
        "  import brax\n",
        "except ImportError:\n",
        "  !pip install git+https://github.com/google/brax.git@main\n",
        "  clear_output()\n",
        "  import brax\n",
        "\n",
        "import flax\n",
        "from brax import envs\n",
        "from brax.io import model\n",
        "from brax.io import json\n",
        "from brax.io import html\n",
        "from brax.training.agents.ppo import train as ppo\n",
        "from brax.training.agents.sac import train as sac\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  from jax.tools import colab_tpu\n",
        "  colab_tpu.setup_tpu()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "B_Yx1cf1C-Z2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ae897cf-e5a3-4a7e-9bfe-7a2162a891ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show brax"
      ],
      "metadata": {
        "id": "KrMhhDdYEdxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install git+https://github.com/waymo-research/waymax.git@main#egg=waymo-waymax"
      ],
      "metadata": {
        "id": "Hu56x3t-EHmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from waymax import config as _config\n",
        "from waymax import dataloader\n",
        "from waymax import dynamics\n",
        "from waymax.env import base_environment\n",
        "from waymax.env import planning_agent_environment\n",
        "from waymax.env.wrappers import brax_wrapper\n",
        "\n",
        "# Initialization\n",
        "single_stateless_env = planning_agent_environment.PlanningAgentEnvironment(\n",
        "        dynamics_model=dynamics.DeltaGlobal(), config=_config.EnvironmentConfig(init_steps=88)\n",
        "    )\n",
        "single_env = brax_wrapper.BraxWrapper(\n",
        "        single_stateless_env,\n",
        "        single_stateless_env.dynamics,\n",
        "        single_stateless_env.config,\n",
        ")\n",
        "\n",
        "print(isinstance(single_env, envs.Env))\n",
        "\"\"\"# Rollout\n",
        "state = waymax_env.reset(next(scenarios))\n",
        "way_env = env.AbstractEnvironment()\n",
        "env = env.brax_wrapper(way_env,dynamics_model,env_config)\"\"\""
      ],
      "metadata": {
        "id": "bcU9oVYdC8s-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "c3376529-1350-448b-fb89-8c19052be866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Rollout\\nstate = waymax_env.reset(next(scenarios))\\nway_env = env.AbstractEnvironment()\\nenv = env.brax_wrapper(way_env,dynamics_model,env_config)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = 'ant'  # @param ['ant', 'halfcheetah', 'hopper', 'humanoid', 'humanoidstandup', 'inverted_pendulum', 'inverted_double_pendulum', 'pusher', 'reacher', 'walker2d']\n",
        "# We determined some reasonable hyperparameters offline and share them here."
      ],
      "metadata": {
        "id": "6WUh8Br-CwfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training\n",
        "\n",
        "train_fn = {\n",
        "  'inverted_pendulum': functools.partial(ppo.train, num_timesteps=2_000_000, num_evals=20, reward_scaling=10, episode_length=1000, normalize_observations=True, action_repeat=1, unroll_length=5, num_minibatches=32, num_updates_per_batch=4, discounting=0.97, learning_rate=3e-4, entropy_cost=1e-2, num_envs=2048, batch_size=1024, seed=1),\n",
        "  'inverted_double_pendulum': functools.partial(ppo.train, num_timesteps=20_000_000, num_evals=20, reward_scaling=10, episode_length=1000, normalize_observations=True, action_repeat=1, unroll_length=5, num_minibatches=32, num_updates_per_batch=4, discounting=0.97, learning_rate=3e-4, entropy_cost=1e-2, num_envs=2048, batch_size=1024, seed=1),\n",
        "  'ant': functools.partial(ppo.train,  num_timesteps=50_000_000, num_evals=10, reward_scaling=10, episode_length=1000),\n",
        "  'humanoid': functools.partial(ppo.train,  num_timesteps=50_000_000, num_evals=10, reward_scaling=0.1, episode_length=1000, normalize_observations=True, action_repeat=1, unroll_length=10, num_minibatches=32, num_updates_per_batch=8, discounting=0.97, learning_rate=3e-4, entropy_cost=1e-3, num_envs=2048, batch_size=1024, seed=1),\n",
        "  'reacher': functools.partial(ppo.train, num_timesteps=50_000_000, num_evals=20, reward_scaling=5, episode_length=1000, normalize_observations=True, action_repeat=4, unroll_length=50, num_minibatches=32, num_updates_per_batch=8, discounting=0.95, learning_rate=3e-4, entropy_cost=1e-3, num_envs=2048, batch_size=256, max_devices_per_host=8, seed=1),\n",
        "  'humanoidstandup': functools.partial(ppo.train, num_timesteps=100_000_000, num_evals=20, reward_scaling=0.1, episode_length=1000, normalize_observations=True, action_repeat=1, unroll_length=15, num_minibatches=32, num_updates_per_batch=8, discounting=0.97, learning_rate=6e-4, entropy_cost=1e-2, num_envs=2048, batch_size=1024, seed=1),\n",
        "  'hopper': functools.partial(sac.train, num_timesteps=6_553_600, num_evals=20, reward_scaling=30, episode_length=1000, normalize_observations=True, action_repeat=1, discounting=0.997, learning_rate=6e-4, num_envs=128, batch_size=512, grad_updates_per_step=64, max_devices_per_host=1, max_replay_size=1048576, min_replay_size=8192, seed=1),\n",
        "  'walker2d': functools.partial(sac.train, num_timesteps=7_864_320, num_evals=20, reward_scaling=5, episode_length=1000, normalize_observations=True, action_repeat=1, discounting=0.997, learning_rate=6e-4, num_envs=128, batch_size=128, grad_updates_per_step=32, max_devices_per_host=1, max_replay_size=1048576, min_replay_size=8192, seed=1),\n",
        "  'halfcheetah': functools.partial(ppo.train, num_timesteps=50_000_000, num_evals=20, reward_scaling=1, episode_length=1000, normalize_observations=True, action_repeat=1, unroll_length=20, num_minibatches=32, num_updates_per_batch=8, discounting=0.95, learning_rate=3e-4, entropy_cost=0.001, num_envs=2048, batch_size=512, seed=3),\n",
        "  'pusher': functools.partial(ppo.train, num_timesteps=50_000_000, num_evals=20, reward_scaling=5, episode_length=1000, normalize_observations=True, action_repeat=1, unroll_length=30, num_minibatches=16, num_updates_per_batch=8, discounting=0.95, learning_rate=3e-4,entropy_cost=1e-2, num_envs=2048, batch_size=512, seed=3),\n",
        "}[env_name]\n",
        "\n",
        "\n",
        "max_y = {'ant': 8000, 'halfcheetah': 8000, 'hopper': 2500, 'humanoid': 13000, 'humanoidstandup': 75_000, 'reacher': 5, 'walker2d': 5000, 'pusher': 0}[env_name]\n",
        "min_y = {'reacher': -100, 'pusher': -150}.get(env_name, 0)\n",
        "\n",
        "xdata, ydata = [], []\n",
        "times = [datetime.now()]\n",
        "\n",
        "\n",
        "def progress(num_steps, metrics):\n",
        "  times.append(datetime.now())\n",
        "  xdata.append(num_steps)\n",
        "  ydata.append(metrics['eval/episode_reward'])\n",
        "  clear_output(wait=True)\n",
        "  plt.xlim([0, train_fn.keywords['num_timesteps']])\n",
        "  plt.ylim([min_y, max_y])\n",
        "  plt.xlabel('# environment steps')\n",
        "  plt.ylabel('reward per episode')\n",
        "  plt.plot(xdata, ydata)\n",
        "  plt.show()\n",
        "\n",
        "make_inference_fn, params, _ = train_fn(environment=single_env, progress_fn=progress)\n",
        "\n",
        "print(f'time to jit: {times[1] - times[0]}')\n",
        "print(f'time to train: {times[-1] - times[1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "8m1eQWlAClg-",
        "outputId": "d5d415f4-ec98-4117-f5a9-5c00f673c7c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "wrap_for_training() got an unexpected keyword argument 'randomization_fn'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-64c6f87df661>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mmake_inference_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msingle_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'time to jit: {times[1] - times[0]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/brax/training/agents/ppo/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(environment, num_timesteps, episode_length, action_repeat, num_envs, max_devices_per_host, num_eval_envs, learning_rate, entropy_cost, discounting, seed, unroll_length, batch_size, num_minibatches, num_updates_per_batch, num_evals, num_resets_per_eval, normalize_observations, reward_scaling, clipping_epsilon, gae_lambda, deterministic_eval, network_factory, progress_fn, normalize_advantage, eval_env, policy_params_fn, randomization_fn)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0mwrap_for_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvs_v1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_for_training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m   env = wrap_for_training(\n\u001b[0;32m--> 215\u001b[0;31m       \u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m       \u001b[0mepisode_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepisode_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m       \u001b[0maction_repeat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: wrap_for_training() got an unexpected keyword argument 'randomization_fn'"
          ]
        }
      ]
    }
  ]
}